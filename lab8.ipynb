{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from art.estimators.classification import KerasClassifier\n",
    "from art.attacks.evasion import FastGradientMethod, DeepFool\n",
    "from art.defences.trainer import AdversarialTrainer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desactivar eager execution\n",
    "tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9339 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "# Cargar y preprocesar los datos\n",
    "path = 'malimg_paper_dataset_imgs'\n",
    "familias = ImageDataGenerator().flow_from_directory(directory=path, target_size=(64, 64), batch_size=10000)\n",
    "imgs, labels = next(familias)\n",
    "imgs_normalized = imgs / 255.0\n",
    "label_binarizer = LabelBinarizer()\n",
    "labels_encoded = label_binarizer.fit_transform(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(imgs_normalized, labels_encoded, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir el modelo\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(25, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el modelo a un clasificador de ART\n",
    "art_model = KerasClassifier(model=model, clip_values=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el ataque FGSM\n",
    "fgsm = FastGradientMethod(estimator=art_model, eps=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precompute adv samples: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Adversarial training epochs: 100%|██████████| 10/10 [03:54<00:00, 23.46s/it]\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo con entrenamiento adversarial\n",
    "adv_trainer = AdversarialTrainer(classifier=art_model, attacks=fgsm, ratio=0.5)\n",
    "adv_trainer.fit(X_train, y_train, nb_epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluación del modelo en ejemplos limpios:\n",
      "Clean test accuracy: 77.16%\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba limpio\n",
    "print(\"\\nEvaluación del modelo en ejemplos limpios:\")\n",
    "clean_accuracy = art_model._model.evaluate(X_test, y_test)\n",
    "print(f\"Clean test accuracy: {clean_accuracy[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples (FGSM): 72.52%\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en ejemplos adversarios FGSM\n",
    "X_test_adv_fgsm = fgsm.generate(x=X_test)\n",
    "predictions_adv_fgsm = art_model.predict(X_test_adv_fgsm)\n",
    "accuracy_adv_fgsm = np.sum(np.argmax(predictions_adv_fgsm, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(f\"Accuracy on adversarial test examples (FGSM): {accuracy_adv_fgsm * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en ejemplos adversarios DeepFool (usando un subconjunto más pequeño y limitando iteraciones)\n",
    "subset_size = 500  # Tamaño del subconjunto para la evaluación\n",
    "X_test_subset = X_test[:subset_size]\n",
    "y_test_subset = y_test[:subset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeepFool: 100%|██████████| 500/500 [10:57<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples (DeepFool): 39.40%\n"
     ]
    }
   ],
   "source": [
    "deepfool = DeepFool(art_model, max_iter=10)  # Limitar el número de iteraciones\n",
    "X_test_adv_deepfool = deepfool.generate(x=X_test_subset)\n",
    "predictions_adv_deepfool = art_model.predict(X_test_adv_deepfool)\n",
    "accuracy_adv_deepfool = np.sum(np.argmax(predictions_adv_deepfool, axis=1) == np.argmax(y_test_subset, axis=1)) / len(y_test_subset)\n",
    "print(f\"Accuracy on adversarial test examples (DeepFool): {accuracy_adv_deepfool * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicación de las Técnicas de Defensa Elegidas\n",
    "\n",
    "## Entrenamiento Adversarial\n",
    "- Esta técnica involucra el entrenamiento del modelo utilizando una combinación de datos originales y datos adversarios generados durante el entrenamiento. Esto mejora la capacidad del modelo para resistir ataques adversarios al enseñarle a reconocer y manejar perturbaciones adversarias.\n",
    "\n",
    "## Detección de Ejemplos Adversarios\n",
    "- Utiliza un detector que identifica ejemplos adversarios basándose en desviaciones en las características del input. Esto es útil para identificar cuándo un ejemplo es potencialmente adversario, permitiendo así tomar medidas adicionales, como bloquear la predicción o aplicar técnicas de corrección."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resultado\n",
    "\n",
    "- La defensa que implementé fue eficaz para proteger el modelo contra adversarios generados mediante el ataque Fast Gradient Sign Method (FGSM) y DeepFool. Al evaluar el modelo en ejemplos limpios, logramos una precisión del 77.16%. Sin embargo, al exponer el modelo a ejemplos adversarios generados mediante FGSM, la precisión disminuyó a 72.52%, lo que indica que algunos ejemplos fueron clasificados incorrectamente. Por otro lado, al limitar el número de iteraciones en el ataque DeepFool y evaluar el modelo en un subconjunto más pequeño de ejemplos, observamos una precisión del 39.40%, lo que sugiere que la defensa pudo mitigar en cierta medida los efectos adversos de este ataque. Estos resultados demuestran la efectividad de la defensa implementada en proteger el modelo contra ataques adversarios, aunque aún existe margen para mejorar la robustez del modelo ante ciertos tipos de ataques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
